# S05 – линейные модели и честный ML-эксперимент (логистическая регрессия, бейзлайн, метрики).

HW05 относится к семинару **S05** и выполняется в личном репозитории студента, созданном на основе шаблона `aie-student-template`, в папке `homeworks/HW05/`.

---


## 1. Структура для HW05

1. В корне репозитория должна быть папка `homeworks/` (создать, если её ещё нет).
2. Внутри `homeworks/` создать папку `HW05/`.
3. В папке `homeworks/HW05/` создать основной ноутбук `HW05.ipynb`.
4. Рекомендуется (но не строго обязательно) создать внутри `homeworks/HW05/` дополнительные подпапки для артефактов эксперимента:
   - `figures/` – для сохранения графиков (ROC-кривая, PR-кривая и т.п.);
   - `artifacts/` – по желанию, для сохранения таблиц с результатами, параметров моделей и т.д.

---

## 2. Учебный датасет `S05-hw-dataset.csv`

1. Использовать **учебный табличный датасет**, предоставленный преподавателем:
   - файл `S05-hw-dataset.csv` будет приложен к материалам семинара S05;
   - рекомендуется поместить его в папку `seminars/S05/` вашего репозитория или в любую другую осмысленную папку данных;
   - в ноутбуке `HW05.ipynb` путь к файлу должен быть **относительным** (без абсолютных путей к домашним каталогам).

2. Все данные в датасете **полностью синтетические** и не описывают реальных людей или реальные кредитные истории.
---

### 1. Общая информация

- Каждая строка – условный клиент банка с набором финансовых и поведенческих признаков.
- Целевая переменная (`target`) – столбец `default`: факт дефолта по кредиту (1 – дефолт, 0 – нет).
- Размер датасета – порядка 3000 наблюдений (строк).
- Доля `default = 1` – около 40% (задача не идеально сбалансирована, но и не экстремально перекошена).
---

### 2. Описание столбцов

В датасете содержатся следующие столбцы (набор может быть немного расширен, но смысл – такой):

- `client_id` – идентификатор клиента (целое число).  
  Используется только как технический ID, в модели его можно не использовать.

- `age` – возраст клиента (целое число, ~21-69 лет).

- `income` – годовой доход клиента (целое число в условных единицах, ~15 000-200 000).

- `years_employed` – стаж работы (количество полных лет, целое число).

- `credit_score` – условный кредитный скоринг (целое число, диапазон примерно 300-850).  
  Чем выше значение, тем «надёжнее» клиент.

- `debt_to_income` – отношение ежемесячных долговых платежей к доходу (вещественное число от 0 до 1).

- `num_credit_cards` – количество кредитных карт (целое число, обычно от 0 до 7).

- `num_late_payments` – количество просрочек платежей за некоторый период (целое число, от 0 и выше).

- `has_mortgage` – флаг наличия ипотеки (0 – нет, 1 – есть).

- `has_car_loan` – флаг наличия автокредита (0 – нет, 1 – есть).

- `savings_balance` – объём сбережений клиента (целое число, от 0 и выше, условные единицы).

- `checking_balance` – баланс на расчетном счёте (целое число, может быть отрицательным).

- `region_risk_score` – условный риск региона проживания клиента (вещественное число 0..1; чем больше, тем рискованнее).

- `phone_calls_to_support_last_3m` – количество обращений клиента в поддержку за последние 3 месяца (целое число).

- `active_loans` – количество активных займов (целое число).

- `customer_tenure_years` – сколько лет клиент обслуживается в этом банке (целое число).

- `default` – **целевой бинарный признак**: факт дефолта по кредиту (0/1).  
---

## 3. Содержание ноутбука `HW05.ipynb` (основная часть)

В ноутбуке `homeworks/HW05/HW05.ipynb` необходимо выполнить следующие шаги.

---

### 1. Загрузка данных и первичный анализ

1. Импортировать необходимые библиотеки:
   - `pandas` и, при необходимости, `numpy`;
   - модули из `scikit-learn`: `train_test_split`, `DummyClassifier`, `LogisticRegression`, `Pipeline`, `StandardScaler`, метрики (`accuracy_score`, `roc_auc_score` и т.п.);
   - при желании – `matplotlib`/`seaborn` для графиков.

2. Загрузить датасет `S05-hw-dataset.csv` в `pandas.DataFrame` с помощью `pd.read_csv`.

3. Вывести и проанализировать:
   - первые строки датасета (`head()`),
   - информацию о столбцах и типах (`info()`),
   - базовые описательные статистики для числовых признаков (`describe()` или аналог),
   - распределение целевого признака `default` (например, через `value_counts(normalize=True)`).

4. Кратко (несколько предложений) зафиксировать наблюдения:
   - сколько объектов и признаков в датасете;
   - есть ли явные аномалии (например, явно невозможные значения);
   - как распределён таргет (баланс классов).

---

### 2. Подготовка признаков и таргета

1. Выделить матрицу признаков `X` и вектор таргета `y`:
   - таргет – столбец `default`;
   - в качестве признаков использовать все остальные осмысленные столбцы (кроме `client_id`, который можно удалить или не использовать в `X`).

2. При необходимости выполнить простую предобработку:
   - убедиться, что все используемые признаки числовые;
   - при желании можно явно проверить диапазоны (например, что `debt_to_income` в [0, 1]).

---

### 3. Train/Test-сплит и бейзлайн-модель

1. Разделить данные на обучающую и тестовую выборки:
   - использовать `train_test_split` из `sklearn.model_selection`;
   - разумное соотношение, например `test_size=0.2` или `0.25`;
   - важно: зафиксировать `random_state` (например, `random_state=42`), чтобы результаты были воспроизводимыми;
   - рекомендуется использовать `stratify=y`, чтобы сохранить баланс классов.

2. Построить **бейзлайн-модель** на основе `DummyClassifier`:
   - например, `strategy="most_frequent"` или `strategy="stratified"`;
   - обучить её на обучающей выборке (`fit(X_train, y_train)`).

3. Оценить бейзлайн по крайней мере по двум метрикам:
   - `accuracy` на тестовой выборке;
   - `ROC-AUC` на тестовой выборке (если используете `predict_proba` или `decision_function`).

4. Вывести значения метрик и коротко прокомментировать, что делает бейзлайн и почему важно иметь точку отсчёта.

---

### 4. Логистическая регрессия и подбор гиперпараметров

1. Построить `Pipeline`, состоящий минимум из:
   - стандартизации признаков (`StandardScaler`);
   - логистической регрессии (`LogisticRegression`).

   Примерно в таком духе (код можно оформить по-своему):

   ```python
   pipe = Pipeline([
       ("scaler", StandardScaler()),
       ("logreg", LogisticRegression(max_iter=1000))
   ])
   ```

2. Подобрать параметр регуляризации `C` (и при желании ещё 1-2 параметра) с помощью:

   - либо `GridSearchCV`;
   - либо простого перебора в цикле по нескольким значениям `C` (например, `[0.01, 0.1, 1.0, 10.0]`).

3. Для лучшей найденной модели посчитать на тестовой выборке:

   - `accuracy`;
   - `ROC-AUC`;

4. Построить хотя бы один график:

   - ROC-кривая **или** PR-кривая (для этого можно использовать функции из `sklearn.metrics` + `matplotlib`).

5. Сохранить хотя бы один график (например, ROC-кривую) в файл в папку `homeworks/HW05/figures/`.

---

### 5. Сравнение бейзлайна и логистической регрессии, текстовые выводы

1. Свести результаты в компактный вид:

   - можно сделать небольшую табличку (например, `pandas.DataFrame`), где по строкам – модели (Dummy vs LogisticRegression), по столбцам – метрики;
   - либо просто аккуратно вывести все значения в текстовом виде.

2. В конце ноутбука написать **краткий текстовый отчёт** (5-10 предложений), в котором:

   - объяснить, чем бейзлайн отличается от логистической регрессии по качеству;
   - указать, насколько сильно выросла (или не выросла) `accuracy` и `ROC-AUC`;
   - при наличии нескольких значений `C` – прокомментировать, как изменение регуляризации влияло на качество;
   - сформулировать 2-3 простых вывода о том, какая модель кажется разумной для этой задачи и почему.

Вывод: В ходе выполнения работы были протестированы две модели: бейзлайн (DummyClassifier) и логистическая регрессия. Бейзлайн (Dummy) использует стратегию "наиболее часто встречающийся класс" и показал точность 0.5893, что является низким результатом. Для логистической регрессии (LogisticRegression) точность составила 0.7973, что существенно выше. Метрика ROC-AUC для логистической регрессии также была выше и составила 0.86, в то время как для бейзлайна она была значительно ниже. При изменении параметра регуляризации C увеличение его значения улучшало точность и ROC-AUC.

---